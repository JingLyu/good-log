SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/tmp/hadoop-root/nm-local-dir/filecache/10/spark-assembly-1.6.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
16/08/13 08:47:41 INFO yarn.ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]
16/08/13 08:47:44 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1471092392707_0002_000001
16/08/13 08:47:46 INFO spark.SecurityManager: Changing view acls to: root
16/08/13 08:47:46 INFO spark.SecurityManager: Changing modify acls to: root
16/08/13 08:47:46 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
16/08/13 08:47:46 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread
16/08/13 08:47:46 INFO yarn.ApplicationMaster: Waiting for spark context initialization
16/08/13 08:47:46 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... 
16/08/13 08:47:47 INFO spark.SparkContext: Running Spark version 1.6.0
16/08/13 08:47:47 INFO spark.SecurityManager: Changing view acls to: root
16/08/13 08:47:47 INFO spark.SecurityManager: Changing modify acls to: root
16/08/13 08:47:47 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
16/08/13 08:47:47 INFO util.Utils: Successfully started service 'sparkDriver' on port 56498.
16/08/13 08:47:48 INFO slf4j.Slf4jLogger: Slf4jLogger started
16/08/13 08:47:48 INFO Remoting: Starting remoting
16/08/13 08:47:48 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.17.0.2:37638]
16/08/13 08:47:48 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 37638.
16/08/13 08:47:48 INFO spark.SparkEnv: Registering MapOutputTracker
16/08/13 08:47:48 INFO spark.SparkEnv: Registering BlockManagerMaster
16/08/13 08:47:48 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1471092392707_0002/blockmgr-edefbad7-10a8-42f0-939e-880af967beb0
16/08/13 08:47:49 INFO storage.MemoryStore: MemoryStore started with capacity 517.4 MB
16/08/13 08:47:49 INFO spark.SparkEnv: Registering OutputCommitCoordinator
16/08/13 08:47:49 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
16/08/13 08:47:49 INFO server.Server: jetty-8.y.z-SNAPSHOT
16/08/13 08:47:49 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:59064
16/08/13 08:47:49 INFO util.Utils: Successfully started service 'SparkUI' on port 59064.
16/08/13 08:47:49 INFO ui.SparkUI: Started SparkUI at http://172.17.0.2:59064
16/08/13 08:47:49 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler
16/08/13 08:47:49 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49464.
16/08/13 08:47:49 INFO netty.NettyBlockTransferService: Server created on 49464
16/08/13 08:47:49 INFO storage.BlockManagerMaster: Trying to register BlockManager
16/08/13 08:47:50 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.2:49464 with 517.4 MB RAM, BlockManagerId(driver, 172.17.0.2, 49464)
16/08/13 08:47:50 INFO storage.BlockManagerMaster: Registered BlockManager
16/08/13 08:47:50 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@172.17.0.2:56498)
16/08/13 08:47:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8030
16/08/13 08:47:50 INFO yarn.YarnRMClient: Registering the ApplicationMaster
16/08/13 08:47:50 INFO yarn.YarnAllocator: Will request 2 executor containers, each with 1 cores and 1408 MB memory including 384 MB overhead
16/08/13 08:47:50 INFO yarn.YarnAllocator: Container request (host: Any, capability: <memory:1408, vCores:1>)
16/08/13 08:47:50 INFO yarn.YarnAllocator: Container request (host: Any, capability: <memory:1408, vCores:1>)
16/08/13 08:47:50 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals
16/08/13 08:47:51 INFO impl.AMRMClientImpl: Received new token for : sandbox:55027
16/08/13 08:47:51 INFO yarn.YarnAllocator: Launching container container_1471092392707_0002_01_000002 for on host sandbox
16/08/13 08:47:51 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@172.17.0.2:56498,  executorHostname: sandbox
16/08/13 08:47:51 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
16/08/13 08:47:51 INFO yarn.ExecutorRunnable: Starting Executor Container
16/08/13 08:47:51 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
16/08/13 08:47:51 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
16/08/13 08:47:51 INFO yarn.ExecutorRunnable: Preparing Local resources
16/08/13 08:47:51 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource { scheme: "hdfs" host: "sandbox" port: 9000 file: "/user/root/.sparkStaging/application_1471092392707_0002/spark-examples-1.6.0-hadoop2.6.0.jar" } size: 118922054 timestamp: 1471092447837 type: FILE visibility: PRIVATE, __spark__.jar -> resource { scheme: "hdfs" host: "sandbox" port: 9000 file: "/spark/spark-assembly-1.6.0-hadoop2.6.0.jar" } size: 187548272 timestamp: 1452328076584 type: FILE visibility: PUBLIC, metrics.properties -> resource { scheme: "hdfs" host: "sandbox" port: 9000 file: "/user/root/.sparkStaging/application_1471092392707_0002/metrics.properties" } size: 6671 timestamp: 1471092447992 type: FILE visibility: PRIVATE)
16/08/13 08:47:51 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>/usr/local/hadoop/etc/hadoop<CPS>/usr/local/hadoop/share/hadoop/common/*<CPS>/usr/local/hadoop/share/hadoop/common/lib/*<CPS>/usr/local/hadoop/share/hadoop/hdfs/*<CPS>/usr/local/hadoop/share/hadoop/hdfs/lib/*<CPS>/usr/local/hadoop/share/hadoop/mapreduce/*<CPS>/usr/local/hadoop/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/share/hadoop/yarn/*<CPS>/usr/local/hadoop/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*
    SPARK_LOG_URL_STDERR -> http://sandbox:8042/node/containerlogs/container_1471092392707_0002_01_000002/root/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1471092392707_0002
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 187548272,118922054,6671
    SPARK_USER -> root
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1452328076584,1471092447837,1471092447992
    SPARK_LOG_URL_STDOUT -> http://sandbox:8042/node/containerlogs/container_1471092392707_0002_01_000002/root/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://sandbox:9000/spark/spark-assembly-1.6.0-hadoop2.6.0.jar#__spark__.jar,hdfs://sandbox:9000/user/root/.sparkStaging/application_1471092392707_0002/spark-examples-1.6.0-hadoop2.6.0.jar#__app__.jar,hdfs://sandbox:9000/user/root/.sparkStaging/application_1471092392707_0002/metrics.properties#metrics.properties

  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.ui.port=0' '-Dspark.driver.port=56498' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.17.0.2:56498 --executor-id 1 --hostname sandbox --cores 1 --app-id application_1471092392707_0002 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
16/08/13 08:47:51 INFO impl.ContainerManagementProtocolProxy: Opening proxy : sandbox:55027
16/08/13 08:47:52 INFO yarn.YarnAllocator: Launching container container_1471092392707_0002_01_000003 for on host sandbox
16/08/13 08:47:52 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@172.17.0.2:56498,  executorHostname: sandbox
16/08/13 08:47:52 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
16/08/13 08:47:52 INFO yarn.ExecutorRunnable: Starting Executor Container
16/08/13 08:47:52 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
16/08/13 08:47:52 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
16/08/13 08:47:52 INFO yarn.ExecutorRunnable: Preparing Local resources
16/08/13 08:47:52 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource { scheme: "hdfs" host: "sandbox" port: 9000 file: "/user/root/.sparkStaging/application_1471092392707_0002/spark-examples-1.6.0-hadoop2.6.0.jar" } size: 118922054 timestamp: 1471092447837 type: FILE visibility: PRIVATE, __spark__.jar -> resource { scheme: "hdfs" host: "sandbox" port: 9000 file: "/spark/spark-assembly-1.6.0-hadoop2.6.0.jar" } size: 187548272 timestamp: 1452328076584 type: FILE visibility: PUBLIC, metrics.properties -> resource { scheme: "hdfs" host: "sandbox" port: 9000 file: "/user/root/.sparkStaging/application_1471092392707_0002/metrics.properties" } size: 6671 timestamp: 1471092447992 type: FILE visibility: PRIVATE)
16/08/13 08:47:52 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>/usr/local/hadoop/etc/hadoop<CPS>/usr/local/hadoop/share/hadoop/common/*<CPS>/usr/local/hadoop/share/hadoop/common/lib/*<CPS>/usr/local/hadoop/share/hadoop/hdfs/*<CPS>/usr/local/hadoop/share/hadoop/hdfs/lib/*<CPS>/usr/local/hadoop/share/hadoop/mapreduce/*<CPS>/usr/local/hadoop/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/share/hadoop/yarn/*<CPS>/usr/local/hadoop/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*
    SPARK_LOG_URL_STDERR -> http://sandbox:8042/node/containerlogs/container_1471092392707_0002_01_000003/root/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1471092392707_0002
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 187548272,118922054,6671
    SPARK_USER -> root
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1452328076584,1471092447837,1471092447992
    SPARK_LOG_URL_STDOUT -> http://sandbox:8042/node/containerlogs/container_1471092392707_0002_01_000003/root/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://sandbox:9000/spark/spark-assembly-1.6.0-hadoop2.6.0.jar#__spark__.jar,hdfs://sandbox:9000/user/root/.sparkStaging/application_1471092392707_0002/spark-examples-1.6.0-hadoop2.6.0.jar#__app__.jar,hdfs://sandbox:9000/user/root/.sparkStaging/application_1471092392707_0002/metrics.properties#metrics.properties

  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.ui.port=0' '-Dspark.driver.port=56498' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.17.0.2:56498 --executor-id 2 --hostname sandbox --cores 1 --app-id application_1471092392707_0002 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
16/08/13 08:47:52 INFO impl.ContainerManagementProtocolProxy: Opening proxy : sandbox:55027
16/08/13 08:47:55 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 0 of them.
16/08/13 08:48:05 INFO cluster.YarnClusterSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (sandbox:47982) with ID 1
16/08/13 08:48:06 INFO storage.BlockManagerMasterEndpoint: Registering block manager sandbox:45656 with 517.4 MB RAM, BlockManagerId(1, sandbox, 45656)
16/08/13 08:48:06 INFO cluster.YarnClusterSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (sandbox:47983) with ID 2
16/08/13 08:48:06 INFO storage.BlockManagerMasterEndpoint: Registering block manager sandbox:33310 with 517.4 MB RAM, BlockManagerId(2, sandbox, 33310)
16/08/13 08:48:06 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
16/08/13 08:48:06 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
16/08/13 08:48:07 INFO spark.SparkContext: Starting job: reduce at SparkPi.scala:36
16/08/13 08:48:07 INFO scheduler.DAGScheduler: Got job 0 (reduce at SparkPi.scala:36) with 2 output partitions
16/08/13 08:48:07 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:36)
16/08/13 08:48:07 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/08/13 08:48:07 INFO scheduler.DAGScheduler: Missing parents: List()
16/08/13 08:48:07 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:32), which has no missing parents
16/08/13 08:48:07 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1888.0 B, free 1888.0 B)
16/08/13 08:48:07 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1202.0 B, free 3.0 KB)
16/08/13 08:48:07 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.0.2:49464 (size: 1202.0 B, free: 517.4 MB)
16/08/13 08:48:07 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
16/08/13 08:48:07 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:32)
16/08/13 08:48:07 INFO cluster.YarnClusterScheduler: Adding task set 0.0 with 2 tasks
16/08/13 08:48:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, sandbox, partition 0,PROCESS_LOCAL, 2078 bytes)
16/08/13 08:48:08 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, sandbox, partition 1,PROCESS_LOCAL, 2078 bytes)
16/08/13 08:48:09 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on sandbox:33310 (size: 1202.0 B, free: 517.4 MB)
16/08/13 08:48:09 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on sandbox:45656 (size: 1202.0 B, free: 517.4 MB)
16/08/13 08:48:09 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1737 ms on sandbox (1/2)
16/08/13 08:48:09 INFO scheduler.DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:36) finished in 1.986 s
16/08/13 08:48:09 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1986 ms on sandbox (2/2)
16/08/13 08:48:09 INFO cluster.YarnClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/08/13 08:48:09 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:36, took 2.500261 s
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
16/08/13 08:48:09 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
16/08/13 08:48:10 INFO ui.SparkUI: Stopped Spark web UI at http://172.17.0.2:59064
16/08/13 08:48:10 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors
16/08/13 08:48:10 INFO cluster.YarnClusterSchedulerBackend: Asking each executor to shut down
16/08/13 08:48:10 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/08/13 08:48:10 INFO storage.MemoryStore: MemoryStore cleared
16/08/13 08:48:10 INFO storage.BlockManager: BlockManager stopped
16/08/13 08:48:10 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
16/08/13 08:48:10 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/08/13 08:48:10 INFO spark.SparkContext: Successfully stopped SparkContext
16/08/13 08:48:10 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/08/13 08:48:10 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/08/13 08:48:10 INFO yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
16/08/13 08:48:10 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
16/08/13 08:48:10 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.
16/08/13 08:48:10 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
16/08/13 08:48:10 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1471092392707_0002
16/08/13 08:48:10 INFO util.ShutdownHookManager: Shutdown hook called
16/08/13 08:48:10 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1471092392707_0002/spark-561e6e94-6947-42cc-9158-9e032fa16fc7
