16/08/15 02:23:58 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]
16/08/15 02:23:58 INFO spark.SecurityManager: Changing view acls to: root,spiderdt
16/08/15 02:23:58 INFO spark.SecurityManager: Changing modify acls to: root,spiderdt
16/08/15 02:23:58 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, spiderdt); users with modify permissions: Set(root, spiderdt)
16/08/15 02:23:59 INFO spark.SecurityManager: Changing view acls to: root,spiderdt
16/08/15 02:23:59 INFO spark.SecurityManager: Changing modify acls to: root,spiderdt
16/08/15 02:23:59 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, spiderdt); users with modify permissions: Set(root, spiderdt)
16/08/15 02:23:59 INFO slf4j.Slf4jLogger: Slf4jLogger started
16/08/15 02:23:59 INFO Remoting: Starting remoting
16/08/15 02:23:59 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@node6:41787]
16/08/15 02:23:59 INFO util.Utils: Successfully started service 'sparkExecutorActorSystem' on port 41787.
16/08/15 02:23:59 INFO storage.DiskBlockManager: Created local directory at /data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/blockmgr-c02d7f8c-888f-4c84-a454-6f298404c925
16/08/15 02:23:59 INFO storage.MemoryStore: MemoryStore started with capacity 1247.3 MB
16/08/15 02:23:59 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.1.6:33328
16/08/15 02:23:59 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
16/08/15 02:23:59 INFO executor.Executor: Starting executor ID 1 on host node6
16/08/15 02:23:59 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34888.
16/08/15 02:23:59 INFO netty.NettyBlockTransferService: Server created on 34888
16/08/15 02:23:59 INFO storage.BlockManagerMaster: Trying to register BlockManager
16/08/15 02:23:59 INFO storage.BlockManagerMaster: Registered BlockManager
16/08/15 02:24:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0
16/08/15 02:24:01 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
16/08/15 02:24:01 INFO executor.Executor: Fetching hdfs://192.168.1.3:9000/tmp/hive/spiderdt/_spark_session_dir/5e849e6d-e5a5-4088-b6cd-dbdd99621bc4/hive-exec-2.1.0.jar with timestamp 1471227839817
16/08/15 02:24:01 INFO util.Utils: Fetching hdfs://192.168.1.3:9000/tmp/hive/spiderdt/_spark_session_dir/5e849e6d-e5a5-4088-b6cd-dbdd99621bc4/hive-exec-2.1.0.jar to /data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/spark-8d2d99e8-e722-4adc-9ae7-9d4c63c0a5dd/fetchFileTemp1967307831066406652.tmp
16/08/15 02:24:02 WARN hdfs.DFSClient: DFSInputStream has been closed already
16/08/15 02:24:02 INFO util.Utils: Copying /data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/spark-8d2d99e8-e722-4adc-9ae7-9d4c63c0a5dd/-19228892321471227839817_cache to /data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000002/./hive-exec-2.1.0.jar
16/08/15 02:24:02 INFO executor.Executor: Adding file:/data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000002/./hive-exec-2.1.0.jar to class loader
16/08/15 02:24:02 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
16/08/15 02:24:02 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 60.7 KB, free 60.7 KB)
16/08/15 02:24:02 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 53 ms
16/08/15 02:24:02 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 204.1 KB, free 264.8 KB)
16/08/15 02:24:02 INFO rdd.HadoopRDD: Input split: Paths:/user/hive/warehouse/ods.db/sample/data/p_date=2012-09-18/p_bisac_code=BUS008000/data.csv:0+3422,/user/hive/warehouse/ods.db/sample/data/p_date=2012-09-18/p_bisac_code=BUS010000/data.csv:0+29406,/user/hive/warehouse/ods.db/sample/data/p_date=2012-09-18/p_bisac_code=BUS019000/data.csv:0+2612,/user/hive/warehouse/ods.db/sample/data/p_date=2012-09-18/p_bisac_code=BUS030000/data.csv:0+34918,/user/hive/warehouse/ods.db/sample/data/p_date=2012-09-18/p_bisac_code=BUS041000/data.csv:0+49754InputFormatClass: org.apache.hadoop.mapred.TextInputFormat

16/08/15 02:24:02 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
16/08/15 02:24:02 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 57.7 KB, free 322.5 KB)
16/08/15 02:24:02 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 11 ms
16/08/15 02:24:02 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/08/15 02:24:02 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 754.6 KB, free 1077.1 KB)
16/08/15 02:24:02 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/08/15 02:24:02 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/08/15 02:24:02 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/08/15 02:24:02 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/08/15 02:24:02 INFO exec.Utilities: PLAN PATH = hdfs://192.168.1.3:9000/tmp/hive/spiderdt/4efc0d91-815e-4229-8b55-1380bfd60c7e/hive_2016-08-15_10-23-37_556_6182104920054704054-1/-mr-10004/08172712-23bb-4bc0-a2fc-8d076b6d2d18/map.xml
16/08/15 02:24:02 INFO exec.SerializationUtilities: Deserializing MapWork using kryo
16/08/15 02:24:02 WARN hdfs.DFSClient: DFSInputStream has been closed already
16/08/15 02:24:02 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file
16/08/15 02:24:02 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start
16/08/15 02:24:02 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length
16/08/15 02:24:02 INFO spark.SparkRecordHandler: maximum memory = 2058354688
16/08/15 02:24:02 INFO spark.SparkRecordHandler: conf classpath = [file:/data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000002/__app__.jar, file:/data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000002/./hive-exec-2.1.0.jar, file:/data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000001/tmp/1471227826541-0/hive-exec-2.1.0.jar]
16/08/15 02:24:02 INFO spark.SparkRecordHandler: thread classpath = [file:/data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000002/__app__.jar, file:/data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000002/./hive-exec-2.1.0.jar, file:/data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000001/tmp/1471227826541-0/hive-exec-2.1.0.jar]
16/08/15 02:24:02 INFO exec.Utilities: PLAN PATH = hdfs://192.168.1.3:9000/tmp/hive/spiderdt/4efc0d91-815e-4229-8b55-1380bfd60c7e/hive_2016-08-15_10-23-37_556_6182104920054704054-1/-mr-10004/08172712-23bb-4bc0-a2fc-8d076b6d2d18/map.xml
16/08/15 02:24:02 INFO exec.MapOperator: Initializing operator MAP[0]
16/08/15 02:24:02 INFO spark.SparkMapRecordHandler: 
<MAP>Id =0
  <Children>null
  <\Children>
  <Parent><\Parent>
<\MAP>
16/08/15 02:24:02 INFO exec.TableScanOperator: Initializing operator TS[0]
16/08/15 02:24:02 INFO exec.SelectOperator: Initializing operator SEL[2]
16/08/15 02:24:02 INFO exec.SelectOperator: SELECT struct<inventory_id:int,biblio_id:int,order_id:int,line_id:int,date:date,source_date:date,pub_date:date,catalog_type:string,source_lcp:double,current_lcp:int,list_price:double,rental_units:double,rental_price:double,rental_bookings:double,site_liq_units:double,site_liq_price:double,site_liq_bookings:double,real_price:double,bisac_code:string,bic_code:string,literal:string,literal_root:string,audience:string,biblio_series:int,bisac_code_series:string,literal_root_series:string,p_date:string,p_bisac_code:string>
16/08/15 02:24:02 INFO exec.GroupByOperator: Initializing operator GBY[3]
16/08/15 02:24:02 INFO exec.ReduceSinkOperator: Initializing operator RS[4]
16/08/15 02:24:02 INFO exec.ReduceSinkOperator: Using tag = -1
16/08/15 02:24:02 INFO exec.Utilities: PLAN PATH = hdfs://192.168.1.3:9000/tmp/hive/spiderdt/4efc0d91-815e-4229-8b55-1380bfd60c7e/hive_2016-08-15_10-23-37_556_6182104920054704054-1/-mr-10004/08172712-23bb-4bc0-a2fc-8d076b6d2d18/map.xml
16/08/15 02:24:02 INFO exec.MapOperator: MAP[0]: records read - 1
16/08/15 02:24:02 INFO spark.SparkRecordHandler: processing 1 rows: used memory = 273141584
16/08/15 02:24:02 INFO exec.MapOperator: MAP[0]: records read - 10
16/08/15 02:24:02 INFO spark.SparkRecordHandler: processing 10 rows: used memory = 273141584
16/08/15 02:24:02 INFO exec.Utilities: PLAN PATH = hdfs://192.168.1.3:9000/tmp/hive/spiderdt/4efc0d91-815e-4229-8b55-1380bfd60c7e/hive_2016-08-15_10-23-37_556_6182104920054704054-1/-mr-10004/08172712-23bb-4bc0-a2fc-8d076b6d2d18/map.xml
16/08/15 02:24:02 INFO exec.MapOperator: MAP[0]: records read - 100
16/08/15 02:24:02 INFO spark.SparkRecordHandler: processing 100 rows: used memory = 273141584
16/08/15 02:24:02 INFO exec.Utilities: PLAN PATH = hdfs://192.168.1.3:9000/tmp/hive/spiderdt/4efc0d91-815e-4229-8b55-1380bfd60c7e/hive_2016-08-15_10-23-37_556_6182104920054704054-1/-mr-10004/08172712-23bb-4bc0-a2fc-8d076b6d2d18/map.xml
16/08/15 02:24:02 INFO exec.Utilities: PLAN PATH = hdfs://192.168.1.3:9000/tmp/hive/spiderdt/4efc0d91-815e-4229-8b55-1380bfd60c7e/hive_2016-08-15_10-23-37_556_6182104920054704054-1/-mr-10004/08172712-23bb-4bc0-a2fc-8d076b6d2d18/map.xml
16/08/15 02:24:02 INFO exec.Utilities: PLAN PATH = hdfs://192.168.1.3:9000/tmp/hive/spiderdt/4efc0d91-815e-4229-8b55-1380bfd60c7e/hive_2016-08-15_10-23-37_556_6182104920054704054-1/-mr-10004/08172712-23bb-4bc0-a2fc-8d076b6d2d18/map.xml
16/08/15 02:24:02 INFO exec.MapOperator: DESERIALIZE_ERRORS:0, RECORDS_IN:664, 
16/08/15 02:24:02 INFO exec.GroupByOperator: Begin Hash Table flush: size = 1
16/08/15 02:24:02 INFO exec.ReduceSinkOperator: keys are [] num distributions: 0
16/08/15 02:24:02 INFO exec.ReduceSinkOperator: RS[4]: records written - 1
16/08/15 02:24:02 INFO exec.ReduceSinkOperator: RS[4]: records written - 1
16/08/15 02:24:02 INFO exec.ReduceSinkOperator: RECORDS_OUT_INTERMEDIATE:1, 
16/08/15 02:24:02 INFO spark.SparkRecordHandler: processed 664 rows: used memory = 273141584
16/08/15 02:24:02 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2276 bytes result sent to driver
16/08/15 02:24:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1
16/08/15 02:24:02 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
16/08/15 02:24:02 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
16/08/15 02:24:02 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
16/08/15 02:24:02 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 118.0 KB, free 1195.1 KB)
16/08/15 02:24:02 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 9 ms
16/08/15 02:24:02 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 401.7 KB, free 1596.8 KB)
16/08/15 02:24:02 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
16/08/15 02:24:02 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@192.168.1.6:33328)
16/08/15 02:24:02 INFO spark.MapOutputTrackerWorker: Got the output locations
16/08/15 02:24:02 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
16/08/15 02:24:02 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
16/08/15 02:24:03 INFO spark.SparkRecordHandler: maximum memory = 2058354688
16/08/15 02:24:03 INFO spark.SparkRecordHandler: conf classpath = [file:/data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000002/__app__.jar, file:/data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000002/__app__.jar, file:/data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000002/./hive-exec-2.1.0.jar]
16/08/15 02:24:03 INFO spark.SparkRecordHandler: thread classpath = [file:/data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000002/__app__.jar, file:/data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000002/__app__.jar, file:/data/yarn/nodemanager/local-dirs/usercache/spiderdt/appcache/application_1470896084881_0043/container_1470896084881_0043_01_000002/./hive-exec-2.1.0.jar]
16/08/15 02:24:03 INFO exec.Utilities: PLAN PATH = hdfs://192.168.1.3:9000/tmp/hive/spiderdt/4efc0d91-815e-4229-8b55-1380bfd60c7e/hive_2016-08-15_10-23-37_556_6182104920054704054-1/-mr-10004/71ffc8c3-ace4-433d-8ef1-469d2dfd75b2/reduce.xml
16/08/15 02:24:03 INFO exec.SerializationUtilities: Deserializing ReduceWork using kryo
16/08/15 02:24:03 WARN hdfs.DFSClient: DFSInputStream has been closed already
16/08/15 02:24:03 INFO spark.SparkReduceRecordHandler: 
<GBY>Id =5
  <Children>
    <FS>Id =7
      <Children>
      <\Children>
      <Parent>Id = 5 null<\Parent>
    <\FS>
  <\Children>
  <Parent><\Parent>
<\GBY>
16/08/15 02:24:03 INFO exec.GroupByOperator: Initializing operator GBY[5]
16/08/15 02:24:03 INFO exec.FileSinkOperator: Initializing operator FS[7]
16/08/15 02:24:03 INFO exec.FileSinkOperator: Using serializer : class org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe[[[B@1af122fb]:[_col0]:[bigint]] and formatter : org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat@36dbc8a9
16/08/15 02:24:03 INFO Configuration.deprecation: mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout
16/08/15 02:24:03 INFO spark.SparkRecordHandler: processing 1 rows: used memory = 307555368
16/08/15 02:24:03 INFO spark.SparkRecordHandler: processed 1 rows: used memory = 307555368
16/08/15 02:24:03 INFO exec.FileSinkOperator: Final Path: FS hdfs://192.168.1.3:9000/tmp/hive/spiderdt/4efc0d91-815e-4229-8b55-1380bfd60c7e/hive_2016-08-15_10-23-37_556_6182104920054704054-1/-mr-10001/.hive-staging_hive_2016-08-15_10-23-37_556_6182104920054704054-1/_tmp.-ext-10002/000000_0
16/08/15 02:24:03 INFO exec.FileSinkOperator: Writing to temp file: FS hdfs://192.168.1.3:9000/tmp/hive/spiderdt/4efc0d91-815e-4229-8b55-1380bfd60c7e/hive_2016-08-15_10-23-37_556_6182104920054704054-1/-mr-10001/.hive-staging_hive_2016-08-15_10-23-37_556_6182104920054704054-1/_task_tmp.-ext-10002/_tmp.000000_0
16/08/15 02:24:03 INFO exec.FileSinkOperator: New Final Path: FS hdfs://192.168.1.3:9000/tmp/hive/spiderdt/4efc0d91-815e-4229-8b55-1380bfd60c7e/hive_2016-08-15_10-23-37_556_6182104920054704054-1/-mr-10001/.hive-staging_hive_2016-08-15_10-23-37_556_6182104920054704054-1/_tmp.-ext-10002/000000_0
16/08/15 02:24:03 INFO exec.FileSinkOperator: FS[7]: records written - 1
16/08/15 02:24:03 INFO exec.FileSinkOperator: FS[7]: records written - 1
16/08/15 02:24:03 INFO exec.FileSinkOperator: RECORDS_OUT_0:1, 
16/08/15 02:24:03 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1211 bytes result sent to driver
