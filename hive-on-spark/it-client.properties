#Spark Context configuration
#Thu Nov 03 23:29:29 CST 2016
hive.spark.client.rpc.threads=8
spark.hadoop.yarn.timeline-service.client.max-retries=30
spark.hadoop.yarn.app.mapreduce.am.command-opts=-Xmx1024m
spark.hadoop.hbase.auth.key.update.interval=86400000
spark.hadoop.yarn.nodemanager.address=0.0.0.0\:0
spark.hadoop.yarn.resourcemanager.connect.max-wait.ms=900000
spark.executor.extraJavaOptions=-Dhive.spark.log.dir\=./target/ 
spark.executor.instances=1
spark.hadoop.yarn.timeline-service.recovery.enabled=false
hive.spark.client.rpc.max.size=52428800
spark.hadoop.yarn.timeline-service.address=0.0.0.0\:10200
spark.hadoop.hbase.server.compactchecker.interval.multiplier=1000
spark.driver.extraJavaOptions=-Dhive.spark.log.dir\=./target/ 
spark.hadoop.hbase.hstore.compaction.kv.max=10
spark.hadoop.hbase.client.max.total.tasks=100
spark.hadoop.hbase.status.published=false
spark.hadoop.yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10
spark.hadoop.zookeeper.znode.parent=/hbase
spark.hadoop.hbase.hstore.bytes.per.checksum=16384
spark.hadoop.hbase.hstore.compactionThreshold=3
spark.hadoop.hbase.rootdir=/tmp/hbase-spiderdt/hbase
spark.hadoop.yarn.nodemanager.container-monitor.interval-ms=3000
spark.hadoop.yarn.nodemanager.recovery.dir=/tmp/hadoop-spiderdt/yarn-nm-recovery
spark.hadoop.hbase.security.exec.permission.checks=false
spark.hadoop.yarn.timeline-service.enabled=false
spark.hadoop.yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10
spark.hadoop.hbase.regionserver.optionalcacheflushinterval=3600000
spark.client.authentication.client_id=79d2bfa6-ce6b-49f6-bbcb-0a801087a253
spark.hadoop.yarn.nodemanager.health-checker.script.timeout-ms=1200000
spark.hadoop.yarn.log-aggregation.retain-check-interval-seconds=-1
spark.hadoop.hbase.zookeeper.property.clientPort=2181
spark.hadoop.hbase.http.staticuser.user=dr.stack
spark.hadoop.yarn.nodemanager.sleep-delay-before-sigkill.ms=250
spark.hadoop.yarn.resourcemanager.address=192.168.1.3\:8032
spark.hadoop.yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1
spark.hadoop.yarn.client.failover-retries=0
spark.hadoop.yarn.sharedcache.cleaner.resource-sleep-ms=0
spark.hadoop.yarn.resourcemanager.keytab=/etc/krb5.keytab
spark.hadoop.hbase.hregion.memstore.block.multiplier=4
spark.hadoop.yarn.app.mapreduce.am.container.log.backups=0
spark.hadoop.yarn.resourcemanager.work-preserving-recovery.enabled=true
spark.hadoop.hbase.master.hfilecleaner.plugins=org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner
spark.client.authentication.secret=690ffffffba7c760ffffffaf0fffffff20ffffffc90ffffffb803270ffffffda376b1140f580ffffff93704e4d4b0ffffffc86c524a0ffffffcfa0ffffffb40ffffffcb360ffffffa4
spark.hadoop.hbase.master.info.port=16010
spark.hadoop.hbase.metrics.showTableName=true
spark.hadoop.hbase.rest.threads.min=2
spark.hadoop.yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400
spark.hadoop.yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging
spark.hadoop.yarn.log-aggregation.retain-seconds=-1
spark.hadoop.yarn.resourcemanager.zk-retry-interval-ms=1000
spark.hadoop.yarn.sharedcache.admin.address=0.0.0.0\:8047
spark.hadoop.yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true
spark.hadoop.yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn
spark.hadoop.hbase.defaults.for.version.skip=false
spark.hadoop.hbase.hstore.blockingWaitTime=90000
spark.hadoop.yarn.client.nodemanager-client-async.thread-pool-max-size=500
spark.hadoop.yarn.acl.enable=false
spark.hadoop.yarn.app.mapreduce.am.container.log.limit.kb=0
spark.hadoop.hbase.coprocessor.abortonerror=true
spark.hadoop.hbase.server.thread.wakefrequency=10000
spark.hadoop.yarn.sharedcache.nm.uploader.replication.factor=10
spark.hadoop.hbase.regionserver.storefile.refresh.period=0
spark.hadoop.hbase.table.max.rowsize=1073741824
spark.hadoop.yarn.resourcemanager.zk-acl=world\:anyone\:rwcda
spark.hadoop.yarn.resourcemanager.zk-num-retries=1000
spark.hadoop.yarn.nodemanager.webapp.address=0.0.0.0\:8042
spark.hadoop.yarn.timeline-service.client.best-effort=false
spark.hadoop.hbase.rs.cacheblocksonwrite=false
spark.hadoop.yarn.resourcemanager.zk-timeout-ms=10000
spark.hadoop.yarn.client.failover-retries-on-socket-timeouts=0
spark.hadoop.yarn.nodemanager.localizer.client.thread-count=5
spark.hadoop.yarn.app.mapreduce.client-am.ipc.max-retries=3
spark.hadoop.yarn.resourcemanager.ha.automatic-failover.enabled=true
spark.hadoop.yarn.app.mapreduce.shuffle.log.limit.kb=0
hive.spark.client.server.connect.timeout=90000
spark.hadoop.yarn.sharedcache.app-checker.class=org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker
spark.hadoop.hbase.status.listener.class=org.apache.hadoop.hbase.client.ClusterStatusListener$MulticastListener
spark.hadoop.hbase.regionserver.catalog.timeout=600000
spark.hadoop.hbase.zookeeper.peerport=2888
spark.hadoop.yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
spark.hadoop.hbase.regionserver.msginterval=3000
spark.hadoop.hbase.http.filter.initializers=org.apache.hadoop.hbase.http.lib.StaticUserWebFilter
spark.hadoop.yarn.resourcemanager.ha.automatic-failover.zk-base-path=/yarn-leader-election
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.hadoop.yarn.nodemanager.recovery.enabled=false
spark.hadoop.hbase.hregion.memstore.flush.size=134217728
spark.hadoop.yarn.sharedcache.client-server.thread-count=50
spark.hadoop.yarn.resourcemanager.max-completed-applications=10000
spark.hadoop.yarn.nodemanager.container-manager.thread-count=20
spark.hadoop.hbase.cells.scanned.per.heartbeat.check=10000
spark.hadoop.hbase.snapshot.enabled=true
spark.hadoop.yarn.resourcemanager.ha.enabled=false
spark.hadoop.yarn.resourcemanager.hostname=192.168.1.3
spark.hadoop.hbase.regionserver.hlog.writer.impl=org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter
spark.hadoop.hbase.client.max.perserver.tasks=5
spark.hadoop.yarn.resourcemanager.amlauncher.thread-count=50
spark.hadoop.hbase.master.loadbalancer.class=org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer
spark.hadoop.hbase.regionserver.checksum.verify=true
spark.hadoop.yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler
spark.hadoop.yarn.resourcemanager.nodemanager.minimum.version=NONE
spark.hadoop.hbase.snapshot.restore.failsafe.name=hbase-failsafe-{snapshot.name}-{restore.timestamp}
spark.hadoop.yarn.scheduler.minimum-allocation-vcores=1
spark.hadoop.yarn.nodemanager.localizer.cache.target-size-mb=10240
spark.hadoop.yarn.app.mapreduce.shuffle.log.separate=true
spark.hadoop.yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000
spark.hadoop.yarn.resourcemanager.scheduler.address=192.168.1.3\:8030
spark.hadoop.yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor
spark.hadoop.hbase.master.port=16000
spark.hadoop.yarn.timeline-service.leveldb-state-store.path=/tmp/hadoop-spiderdt/yarn/timeline
spark.hadoop.yarn.resourcemanager.admin.address=192.168.1.3\:8033
spark.hadoop.yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true
spark.hadoop.yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
spark.hadoop.yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false
spark.hadoop.yarn.log-aggregation-enable=false
spark.hadoop.hbase.master.distributed.log.replay=false
spark.hadoop.hbase.regionserver.info.port.auto=false
spark.hadoop.yarn.sharedcache.nm.uploader.thread-count=20
spark.hadoop.hbase.ipc.server.callqueue.scan.ratio=0
spark.hadoop.yarn.resourcemanager.connect.retry-interval.ms=30000
spark.hadoop.hbase.zookeeper.quorum=localhost
spark.hadoop.yarn.sharedcache.admin.thread-count=1
spark.hadoop.yarn.sharedcache.store.in-memory.check-period-mins=720
spark.hadoop.yarn.nodemanager.health-checker.interval-ms=600000
spark.hadoop.hbase.thrift.maxWorkerThreads=1000
spark.hadoop.hbase.hstore.blockingStoreFiles=10
spark.hadoop.yarn.sharedcache.nested-level=3
spark.hadoop.yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25
spark.hadoop.yarn.app.mapreduce.shuffle.log.backups=0
spark.hadoop.yarn.nodemanager.localizer.fetch.thread-count=4
spark.hadoop.hbase.region.replica.replication.enabled=false
spark.hadoop.hbase.bulkload.retries.number=10
spark.hadoop.hbase.client.pause=100
spark.hadoop.yarn.nodemanager.remote-app-log-dir=/tmp/logs
spark.hadoop.yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000
spark.hadoop.yarn.nm.liveness-monitor.expiry-interval-ms=600000
spark.hadoop.yarn.nodemanager.disk-health-checker.interval-ms=120000
spark.hadoop.yarn.resourcemanager.system-metrics-publisher.enabled=false
spark.hadoop.yarn.sharedcache.enabled=false
spark.master=yarn-client
spark.hadoop.yarn.sharedcache.cleaner.period-mins=1440
spark.hadoop.hbase.config.read.zookeeper.config=false
spark.hadoop.hbase.ipc.server.callqueue.handler.factor=0.1
spark.hadoop.yarn.resourcemanager.resource-tracker.address=192.168.1.3\:8031
spark.hadoop.yarn.nodemanager.log-aggregation.compression-type=none
spark.hadoop.yarn.nodemanager.process-kill-wait.ms=2000
spark.hadoop.hbase.regionserver.regionSplitLimit=1000
spark.hadoop.hbase.hregion.memstore.mslab.enabled=true
spark.hadoop.yarn.nodemanager.resource.percentage-physical-cpu-limit=100
spark.hadoop.yarn.app.mapreduce.client.max-retries=3
spark.hadoop.hbase.column.max.version=1
spark.hadoop.hbase.cluster.distributed=false
spark.hadoop.hbase.zookeeper.property.maxClientCnxns=300
spark.yarn.queue=default
spark.executor.memory=2g
spark.hadoop.yarn.nodemanager.vmem-pmem-ratio=2.1
spark.hadoop.hbase.rest.threads.max=100
spark.hadoop.yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider
spark.driver.memory=2g
spark.hadoop.hbase.local.dir=/tmp/hbase-spiderdt/local/
spark.hadoop.yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000
spark.hadoop.yarn.nodemanager.log.retain-seconds=10800
spark.hadoop.hbase.dfs.client.read.shortcircuit.buffer.size=131072
spark.hadoop.yarn.resourcemanager.am.max-attempts=2
spark.hadoop.hbase.hregion.max.filesize=10737418240
spark.hadoop.hbase.defaults.for.version=1.1.1
spark.hadoop.hbase.server.versionfile.writeattempts=3
spark.hadoop.hbase.ipc.server.callqueue.read.ratio=0
spark.hadoop.yarn.scheduler.maximum-allocation-mb=8192
spark.hadoop.yarn.timeline-service.ttl-enable=true
spark.hadoop.yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody
spark.hadoop.yarn.app.mapreduce.am.job.committer.commit-window=10000
spark.hadoop.hbase.http.max.threads=10
spark.hadoop.yarn.resourcemanager.ha.automatic-failover.embedded=true
spark.hadoop.yarn.app.mapreduce.am.job.committer.cancel-timeout=60000
spark.hadoop.yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs
spark.hadoop.hbase.table.lock.enable=true
spark.hadoop.hbase.security.visibility.mutations.checkauths=false
spark.hadoop.hbase.thrift.htablepool.size.max=1000
spark.hadoop.hbase.status.publisher.class=org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher
spark.hadoop.hbase.ipc.client.tcpnodelay=true
spark.hadoop.yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$
spark.hadoop.hbase.security.authentication=simple
spark.hadoop.hbase.regionserver.info.port=16030
spark.hadoop.yarn.resourcemanager.admin.client.thread-count=1
spark.hadoop.yarn.app.mapreduce.client.job.retry-interval=2000
spark.hadoop.hbase.rest.filter.classes=org.apache.hadoop.hbase.rest.filter.GzipFilter
spark.hadoop.yarn.resourcemanager.webapp.https.address=192.168.1.3\:8090
spark.hadoop.yarn.app.mapreduce.client.job.max-retries=0
spark.hadoop.yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false
spark.hadoop.zookeeper.znode.rootserver=root-region-server
spark.kryo.classesToRegister=org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch,org.apache.hadoop.io.Writable,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.io.HiveKey
spark.hadoop.hbase.regionserver.dns.nameserver=default
spark.hadoop.yarn.nodemanager.windows-container.cpu-limit.enabled=false
spark.hadoop.yarn.sharedcache.root-dir=/sharedcache
spark.hadoop.hbase.master.catalog.timeout=600000
spark.hadoop.hbase.client.keyvalue.maxsize=10485760
spark.hadoop.hbase.storescanner.parallel.seek.enable=false
spark.hadoop.hbase.regions.slop=0.2
spark.hadoop.hbase.rpc.shortoperation.timeout=10000
spark.hadoop.yarn.resourcemanager.leveldb-state-store.path=/tmp/hadoop-spiderdt/yarn/system/rmstore
spark.hadoop.yarn.admin.acl=*
spark.hadoop.yarn.nodemanager.hostname=0.0.0.0
spark.hadoop.hbase.lease.recovery.timeout=900000
spark.hadoop.yarn.timeline-service.keytab=/etc/krb5.keytab
spark.hadoop.yarn.timeline-service.client.retry-interval-ms=1000
spark.hadoop.yarn.client.nodemanager-connect.max-wait-ms=180000
spark.kryo.referenceTracking=false
spark.hadoop.yarn.resourcemanager.resource-tracker.client.thread-count=50
spark.hadoop.hbase.master.infoserver.redirect=true
hive.spark.client.connect.timeout=1000
spark.hadoop.yarn.resourcemanager.scheduler.monitor.enable=false
spark.hadoop.hbase.coprocessor.enabled=true
spark.hadoop.yarn.sharedcache.store.class=org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore
spark.hadoop.yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000
spark.hadoop.yarn.resourcemanager.recovery.enabled=false
spark.hadoop.yarn.sharedcache.store.in-memory.staleness-period-mins=10080
spark.hadoop.yarn.nodemanager.docker-container-executor.exec-name=/usr/bin/docker
spark.hadoop.hbase.regionserver.port=16020
spark.hadoop.hbase.auth.token.max.lifetime=604800000
spark.hadoop.hbase.hstore.checksum.algorithm=CRC32
spark.hadoop.yarn.timeline-service.leveldb-timeline-store.path=/tmp/hadoop-spiderdt/yarn/timeline
spark.hadoop.hbase.client.scanner.max.result.size=2097152
spark.hadoop.hbase.status.multicast.address.ip=226.1.1.3
spark.hadoop.yarn.app.mapreduce.am.job.task.listener.thread-count=30
spark.hadoop.hbase.client.localityCheck.threadPoolSize=2
spark.hadoop.hbase.storescanner.parallel.seek.threads=10
spark.hadoop.yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore
spark.hadoop.yarn.bin.path=/home/spiderdt/work/git/spiderdt-env/cluster/tarball/hadoop-2.7.1/bin/yarn
spark.hadoop.yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000
spark.hadoop.yarn.nodemanager.pmem-check-enabled=true
spark.hadoop.hbase.regionserver.thrift.compact=false
spark.hadoop.yarn.timeline-service.ttl-ms=604800000
spark.hadoop.hbase.tmp.dir=/tmp/hbase-spiderdt
spark.hadoop.yarn.sharedcache.client-server.address=0.0.0.0\:8045
spark.hadoop.hbase.regionserver.handler.abort.on.error.percent=0.5
spark.hadoop.yarn.timeline-service.http-authentication.type=simple
spark.hadoop.hbase.master.logcleaner.ttl=600000
spark.hadoop.hbase.rootdir.perms=700
spark.hadoop.yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600
spark.hadoop.hbase.bulkload.staging.dir=/user/spiderdt/hbase-staging
spark.hadoop.hbase.regionserver.handler.count=30
spark.hadoop.yarn.resourcemanager.container.liveness-monitor.interval-ms=600000
spark.hadoop.yarn.nodemanager.local-dirs=/tmp/hadoop-spiderdt/nm-local-dir
spark.hadoop.hbase.regionserver.logroll.errors.tolerated=2
spark.hadoop.yarn.timeline-service.webapp.https.address=0.0.0.0\:8190
spark.hadoop.yarn.app.mapreduce.am.resource.cpu-vcores=1
spark.hadoop.yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler
spark.hadoop.hbase.hregion.percolumnfamilyflush.size.lower.bound=16777216
spark.hadoop.yarn.nodemanager.resource.memory-mb=8192
spark.hadoop.yarn.sharedcache.cleaner.initial-delay-mins=10
spark.hadoop.hbase.server.scanner.max.result.size=104857600
spark.hadoop.yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000
spark.hadoop.yarn.sharedcache.webapp.address=0.0.0.0\:8788
spark.hadoop.hbase.hstore.compaction.max=10
spark.hadoop.yarn.client.max-cached-nodemanagers-proxies=0
spark.hadoop.yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400
spark.hadoop.hbase.coprocessor.user.enabled=true
spark.hadoop.yarn.resourcemanager.state-store.max-completed-applications=10000
spark.hadoop.hbase.hregion.majorcompaction=604800000
spark.hadoop.yarn.am.liveness-monitor.expiry-interval-ms=600000
spark.hadoop.hbase.regionserver.thrift.framed.max_frame_size_in_mb=2
spark.hadoop.yarn.sharedcache.store.in-memory.initial-delay-mins=10
spark.hadoop.hbase.hstore.flusher.count=2
spark.hadoop.yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0
spark.hadoop.yarn.scheduler.maximum-allocation-vcores=32
spark.hadoop.yarn.timeline-service.hostname=0.0.0.0
spark.hadoop.yarn.resourcemanager.scheduler.client.thread-count=50
spark.hadoop.yarn.resourcemanager.zk-state-store.parent-path=/rmstore
spark.hadoop.hbase.rest.readonly=false
spark.hadoop.hbase.metrics.exposeOperationTimes=true
spark.hadoop.yarn.nodemanager.vmem-check-enabled=true
spark.hadoop.yarn.nodemanager.local-cache.max-files-per-directory=8192
spark.hadoop.hbase.hregion.preclose.flush.size=5242880
spark.hadoop.hbase.zookeeper.dns.interface=default
spark.hadoop.yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy
spark.hadoop.hbase.client.write.buffer=2097152
spark.app.name=Hive on Spark
spark.hadoop.yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000
spark.hadoop.yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000
spark.hadoop.yarn.nodemanager.windows-container.memory-limit.enabled=false
spark.hadoop.hbase.data.umask=000
spark.hadoop.yarn.nodemanager.linux-container-executor.cgroups.mount=false
spark.hadoop.yarn.nodemanager.remote-app-log-dir-suffix=logs
spark.hadoop.yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider
spark.hadoop.hbase.master.info.bindAddress=0.0.0.0
spark.hadoop.yarn.timeline-service.http-authentication.simple.anonymous.allowed=true
spark.hadoop.yarn.nodemanager.delete.debug-delay-sec=0
spark.hadoop.hbase.regionserver.thrift.framed=false
spark.hadoop.hbase.replication.rpc.codec=org.apache.hadoop.hbase.codec.KeyValueCodecWithTags
spark.hadoop.yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0
spark.hadoop.yarn.scheduler.minimum-allocation-mb=1024
spark.hadoop.hbase.client.retries.number=35
spark.hadoop.hbase.hregion.majorcompaction.jitter=0.50
spark.hadoop.hbase.client.scanner.caching=2147483647
spark.hadoop.hbase.rest.port=8080
spark.hadoop.yarn.nodemanager.keytab=/etc/krb5.keytab
spark.hadoop.yarn.app.mapreduce.am.resource.mb=1536
spark.hadoop.hbase.data.umask.enable=false
spark.hadoop.hbase.thrift.maxQueuedRequests=1000
spark.hadoop.yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME
spark.hadoop.yarn.nodemanager.delete.thread-count=4
spark.hadoop.hbase.thrift.minWorkerThreads=16
spark.hadoop.hbase.dynamic.jars.dir=/tmp/hbase-spiderdt/hbase/lib
spark.hadoop.yarn.timeline-service.state-store-class=org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore
spark.hadoop.yarn.resourcemanager.fs.state-store.retry-interval-ms=1000
spark.hadoop.yarn.resourcemanager.webapp.address=192.168.1.3\:8088
spark.hadoop.yarn.resourcemanager.fs.state-store.retry-policy-spec=2000, 500
spark.hadoop.yarn.nodemanager.localizer.address=0.0.0.0\:8040
spark.hadoop.yarn.timeline-service.handler-thread-count=10
spark.hadoop.hbase.zookeeper.leaderport=3888
spark.hadoop.yarn.resourcemanager.fs.state-store.uri=/tmp/hadoop-spiderdt/yarn/system/rmstore
spark.hadoop.hbase.lease.recovery.dfs.timeout=64000
spark.hadoop.hbase.regionserver.info.bindAddress=0.0.0.0
spark.hadoop.hbase.hstore.time.to.purge.deletes=0
spark.hadoop.yarn.client.nodemanager-connect.retry-interval-ms=10000
spark.hadoop.yarn.resourcemanager.proxy-user-privileges.enabled=false
spark.hadoop.hbase.zookeeper.property.dataDir=/tmp/hbase-spiderdt/zookeeper
spark.hadoop.hbase.rest.support.proxyuser=false
spark.hadoop.yarn.sharedcache.uploader.server.thread-count=50
hive.spark.client.secret.bits=256
spark.hadoop.hbase.snapshot.restore.take.failsafe.snapshot=true
spark.hadoop.hbase.rpc.timeout=60000
spark.hadoop.hbase.zookeeper.property.syncLimit=5
spark.hadoop.yarn.app.mapreduce.task.container.log.backups=0
spark.hadoop.yarn.nodemanager.admin-env=MALLOC_ARENA_MAX\=$MALLOC_ARENA_MAX
spark.hadoop.hbase.client.max.perregion.tasks=1
spark.hadoop.hbase.zookeeper.dns.nameserver=default
spark.hadoop.hbase.coordinated.state.manager.class=org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager
spark.hadoop.yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3
spark.hadoop.yarn.sharedcache.checksum.algo.impl=org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl
spark.hadoop.hbase.status.multicast.address.port=16100
spark.hadoop.hbase.zookeeper.useMulti=true
spark.hadoop.yarn.client.application-client-protocol.poll-interval-ms=200
spark.hadoop.hbase.online.schema.update.enable=true
spark.hadoop.yarn.sharedcache.uploader.server.address=0.0.0.0\:8046
spark.hadoop.hbase.ipc.client.fallback-to-simple-auth-allowed=false
spark.hadoop.yarn.resourcemanager.client.thread-count=50
spark.hadoop.hbase.regionserver.dns.interface=default
spark.hadoop.yarn.resourcemanager.fs.state-store.num-retries=0
spark.hadoop.yarn.app.mapreduce.am.hard-kill-timeout-ms=10000
spark.hadoop.yarn.nodemanager.resource.cpu-vcores=8
spark.hadoop.hbase.master.logcleaner.plugins=org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner
spark.hadoop.yarn.http.policy=HTTP_ONLY
spark.hadoop.hbase.regionserver.logroll.period=3600000
spark.hadoop.yarn.timeline-service.webapp.address=0.0.0.0\:8188
spark.hadoop.hbase.regionserver.region.split.policy=org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy
spark.hadoop.yarn.resourcemanager.nodemanager-connect-retries=10
spark.hadoop.yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
spark.hadoop.hbase.client.scanner.timeout.period=60000
spark.hadoop.hbase.regionserver.hlog.reader.impl=org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader
spark.hadoop.hbase.fs.tmp.dir=/user/spiderdt/hbase-staging
spark.hadoop.fs.defaultFS=hdfs\://192.168.1.3\:9000
spark.hadoop.yarn.nodemanager.resourcemanager.minimum.version=NONE
spark.hadoop.zookeeper.znode.acl.parent=acl
spark.yarn.report.interval=60000
spark.hadoop.hbase.balancer.period=300000
spark.hadoop.hbase.zookeeper.property.initLimit=10
